# -*- coding: utf-8 -*-
"""modeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Myhn_dWVvudfI2u6f26_RoiA4o0hTzHD
"""

! pip install geopandas
! pip install geodatasets
! pip install geoplot

import csv
import pandas as pd
import numpy as np
import geopandas
from geodatasets import get_path
import os

from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.metrics import r2_score

from plotly import express as px
import datetime
from matplotlib import pyplot as plt
import pathlib
from urllib.request import urlopen
import json
with urlopen('https://raw.githubusercontent.com/codeforgermany/click_that_hood/main/public/data/brazil-states.geojson') as response:
    counties = json.load(response)

pd.set_option('display.max_columns', None)

# Conecta o serviço com o Drive
from google.colab import drive
drive.mount('/content/drive')

# Load the utilities sets
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_ENERGISA_SERGIPE.json", 'r') as f:
  test_set_json = json.load(f)
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_LIGHT.json", 'r') as f:
  test_set_json_light = json.load(f)
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_CEMIG.json", 'r') as f:
  test_set_json_cemig = json.load(f)
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_COPEL.json", 'r') as f:
  test_set_json_copel = json.load(f)
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_CPFL.json", 'r') as f:
  test_set_json_cpfl = json.load(f)

os.environ['ROOT'] = '/content/drive/MyDrive/Mestrado/Ordens_servico/'
base_path = os.environ.get("ROOT")

# Raiz
! ls "${ROOT}/Ordens_servico/"

dir_path = pathlib.Path(base_path)

label = "FLAG_INTERRUPCOES"
# Log transmed label variable
# label = "FLAG_INTERRUPCOES_lognorm"

# cols = [
#        'LONGITUDE', 'LATITUDE', "AREA_KM2", "MUN_LENGTH",
#         "MUN_MINIMUM_BOUND_RADIO",
#         # "MUN_NUMBER_OF_BOUNDARYS", # Number of bounderies need a adjust to represent all municipios
#         'Pop estimada 2021',
#        'Faixa_pop', 'Região Metropolitana', 'Nome da Grande Região',
#        'Nome da Mesorregião', 'Nome da Microrregião',
#        'Nome da Região Geográfica Imediata',
#        'Município da Região Geográfica Imediata',
#        'Nome da Região Geográfica Intermediária',
#        'Município da Região Geográfica Intermediária',
#        # 'Nome Concentração Urbana', Retirado por parecer ter muitas classificacoes
#         'Tipo Concentração Urbana',
#        # 'Nome Arranjo Populacional', Retirado por parecer ter muitas classificacoes
#         'Hierarquia Urbana',
#        'Hierarquia Urbana (principais categorias)', 'Nome da Região Rural',
#        'Região rural (segundo classificação do núcleo)', 'Amazônia Legal',
#        'Semiárido',
#         'Cidade-Região de São Paulo',
#       #  'Valor adicionado bruto total, \na preços correntes\n(R$ 1.000)',
#       #  'Impostos, líquidos de subsídios, sobre produtos, \na preços correntes\n(R$ 1.000)',
#        'Produto Interno Bruto, \na preços correntes\n(R$ 1.000)',
#       #  'Produto Interno Bruto per capita, \na preços correntes\n(R$ 1,00)',
#        'FLAG_LIT',
#        'MAX_TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)',
#        'MAX_TEMPERATURA DO PONTO DE ORVALHO (°C)',
#        # 'MAX_UMIDADE RELATIVA DO AR, HORARIA (%)', Retirado por conter muitos registros nulos
#        'MAX_VENTO, DIREÇÃO HORARIA (gr) (° (gr))',
#        'MAX_VENTO, RAJADA MAXIMA (m/s)', 'MAX_VENTO, VELOCIDADE HORARIA (m/s)',
#        'MAX_PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', 'MAX_RADIACAO GLOBAL (Kj/m²)',
#        'MED_TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)',
#        'MED_TEMPERATURA DO PONTO DE ORVALHO (°C)',
#        # 'MED_UMIDADE RELATIVA DO AR, HORARIA (%)', Retirado por conter muitos registros nulos
#        'MED_VENTO, DIREÇÃO HORARIA (gr) (° (gr))',
#        'MED_VENTO, RAJADA MAXIMA (m/s)', 'MED_VENTO, VELOCIDADE HORARIA (m/s)',
#        'MED_PRECIPITAÇÃO TOTAL, HORÁRIO (mm)', 'MED_RADIACAO GLOBAL (Kj/m²)',
#        'TOTAL_PRECIPITAÇÃO TOTAL, HORÁRIO (mm)',
#        'TOTAL_RADIACAO GLOBAL (Kj/m²)',
#        'FLAG_INTERRUPCOES']

cols = [
       "mes",
       'MAX_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)',
       'TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)',
       'MED_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)',
       'MAX_PRESSAO ATMOSFERICA MEDIA DIARIA (AUT)(mB)',
       'MAX_TEMPERATURA DO PONTO DE ORVALHO MEDIA DIARIA (AUT)(Â°C)',
       'MAX_TEMPERATURA MAXIMA, DIARIA (AUT)(Â°C)',
       'MAX_TEMPERATURA MEDIA, DIARIA (AUT)(Â°C)',
       'MAX_TEMPERATURA MINIMA, DIARIA (AUT)(Â°C)',
       'MAX_UMIDADE RELATIVA DO AR, MEDIA DIARIA (AUT)(%)',
       'MAX_UMIDADE RELATIVA DO AR, MINIMA DIARIA (AUT)(%)',
       'MAX_VENTO, RAJADA MAXIMA DIARIA (AUT)(m/s)',
       'MAX_VENTO, VELOCIDADE MEDIA DIARIA (AUT)(m/s)',
       'MED_PRESSAO ATMOSFERICA MEDIA DIARIA (AUT)(mB)',
       'MED_TEMPERATURA DO PONTO DE ORVALHO MEDIA DIARIA (AUT)(Â°C)',
       'MED_TEMPERATURA MAXIMA, DIARIA (AUT)(Â°C)',
       'MED_TEMPERATURA MEDIA, DIARIA (AUT)(Â°C)',
       'MED_TEMPERATURA MINIMA, DIARIA (AUT)(Â°C)',
       'MED_UMIDADE RELATIVA DO AR, MEDIA DIARIA (AUT)(%)',
       'MED_UMIDADE RELATIVA DO AR, MINIMA DIARIA (AUT)(%)',
       'MED_VENTO, RAJADA MAXIMA DIARIA (AUT)(m/s)',
       'MED_VENTO, VELOCIDADE MEDIA DIARIA (AUT)(m/s)',
       'UF',
       #  'Pop estimada 2021', # Retirada por teste somente com variaveis meteorologicas
       #  'Faixa_pop',  # Retirada por teste somente com variaveis meteorologicas
       #'Região Metropolitana',   Retirado por parecer ter muitas classificacoes
       #  'Regiao', # Retirada por teste somente com variaveis meteorologicas
      # 'Nome da Grande Região', Retirado por parecer ter muitas classificacoes
      #  'Nome da Mesorregião',
       # 'Nome da Microrregião',  Retirado por parecer ter muitas classificacoes
      #  'Nome da Região Geográfica Imediata',  # Retirado por parecer ter muitas classificacoes
      #  'Município da Região Geográfica Imediata', # Retirado por parecer ter muitas classificacoes
      #  'Nome da Região Geográfica Intermediária', # Retirado por parecer ter muitas classificacoes
      #  'Município da Região Geográfica Intermediária',  # Retirado por parecer ter muitas classificacoes
       # 'Nome Concentração Urbana',  Retirado por parecer ter muitas classificacoes
      #  'Tipo Concentração Urbana',
       # 'Nome Arranjo Populacional', Retirado por parecer ter muitas classificacoes
      #  'Hierarquia Urbana', Retirado por parecer ter muitas classificacoes
      #  'Hierarquia Urbana (principais categorias)',
       # 'Nome da Região Rural',  Retirado por parecer ter muitas classificacoes
      #  'Região rural (segundo classificação do núcleo)',  Retirado por parecer ter muitas classificacoes
       'Amazônia Legal',
       'Semiárido',
      #  'Cidade-Região de São Paulo',  # Funcao de one-hot-encoding tendo pico de memoria com essa caracteristica
      #  'Valor adicionado bruto total, \na preços correntes\n(R$ 1.000)',
      #  'Impostos, líquidos de subsídios, sobre produtos, \na preços correntes\n(R$ 1.000)',
      #  'Produto Interno Bruto, \na preços correntes\n(R$ 1.000)',
      #  'Produto Interno Bruto per capita, \na preços correntes\n(R$ 1,00)',
       'FLAG_LIT',
       'MUN_MINIMUM_BOUND_RADIO',
       'AREA',
      #  'MUN_NUMBER_OF_BOUNDARYS', Retirada por contar multi tipos de caracteristicas (eg. ste (multipoligono) e float)
       'NUMERO_CONSUMIDORES_DO_CONJUNTO',
       'FLAG_INTERRUPCOES',
       'veg_class',
       "labels",
       "percent_tree_cover"
]

# index = ["NOME_MUNICIPIO",
#          "mes",
#          "year_month",
#          "UF",
#          "GEOCODIGO_MUNICIPIO",
#          "NUMERO_UNIDADES_CONSUMIDORAS",
#         ]

index = ["year_month",
         "DscConjUndConsumidoras",
        ]

def treat_outliers(df):
  # Create a year index
  df["year_month"] = (df["year"]*100 + df["mes"] ).astype('str')
  # inter_quantile = df.FLAG_INTERRUPCOES.quantile(0.5) - df.FLAG_INTERRUPCOES.quantile(0.25)
  # lim_hig = df.FLAG_INTERRUPCOES.quantile(0.75) + inter_quantile * 1.5
  # lim_bot = df.FLAG_INTERRUPCOES.quantile(0.25) - inter_quantile * 1.5
  # Retira casos com baixa quantidade de interrupcoes.
  lim_low_int = df.FLAG_INTERRUPCOES.quantile(0.1)

  print("quantidade registros extremos retirados")
  # outliers = (df.FLAG_INTERRUPCOES < lim_bot) | (df.FLAG_INTERRUPCOES > lim_hig) | (df.FLAG_INTERRUPCOES < lim_low_int)
  outliers = (df.FLAG_INTERRUPCOES < lim_low_int)
  df[outliers].shape[0]

  df = df[~outliers]
  return df

def fill_null(df):
  df.loc[df["Região Metropolitana"].isnull(), "Região Metropolitana"] = "Região Não Metropolitana"
  df.loc[df["Tipo Concentração Urbana"].isnull(), "Tipo Concentração Urbana"] = "Sem Classificacao"
  return df

def add_clusters_info(df):

  clusters_grps = pd.read_csv(base_path + "Ordens_servico/cluster_grps.csv")
  # Transform the clusters labels to string, the model receive it as a categorical data
  clusters_grps["labels"] = clusters_grps["labels"].astype("str")
  df = df.merge(clusters_grps, left_on="DscConjUndConsumidoras", right_on="DscConjuntoUnidadeConsumidora", how="left")
  return df

def select_features(df, index, cols):
  df = df.set_index(index)[cols]
  return df

# Verificar se por algum motivo o conjunto de dados esta contendo index repetidos
def drop_duplic(df):
  df = df.reset_index().drop_duplicates(df.index.names).set_index(df.index.names)
  return df

# Teste - Passa o mes para variavel categorica
def transform_month(df):
  df["mes"] = df["mes"].astype('str')
  return df

def one_hot_econding(df):
  # Input–Output Mapping (Haykin, 1999) das variaveis categoricas
  # Survey on categorical data for neural networks
  # https://brendanhasz.github.io/2019/03/04/target-encoding

  # Encode all categorical cols by default
  cat_cols = df.select_dtypes(include=['object']).columns

  for ind, col in enumerate(cat_cols):
    print(ind)
    print(col)
    temp_df = pd.get_dummies(df[col], drop_first=True, prefix=col)
    if ind == 0:
      one_hot_encoding_df = temp_df
    else:
      one_hot_encoding_df = one_hot_encoding_df.join(temp_df)

  # Une o conjunto com colunas numericas com o conjunto que foi criado co colunas categoricas processadas
  numeric_cols = df.select_dtypes(include=['number']).columns

  df = df[numeric_cols].join(one_hot_encoding_df)

  # def TargetEncoderLOO(df):

  return df

# Completa os dados nulos utilizando as medias, como para o exemplo selecionado sao poucos dados parece fazer sentido o tratamento
def fill_null_mean_tratment(df):
  numeric_cols = df.select_dtypes(include=['number']).columns
  integer_null_cols_dict = df[numeric_cols].isnull().any(axis=0).to_dict()
  for col, value in integer_null_cols_dict.items():
    if value == True:
      df.loc[df[col].isnull(), col] = df[col].mean()

  return df

# Ordena os indexs para realizar o lag
def time_shift(df):
  df = df.reset_index()

  df["mes"] = df.year_month.astype('str').str[4 :].astype('int')
  df["year"] = df.year_month.astype('str').str[0: 4].astype('int')

  df = df.sort_values(['year', 'mes'])
  # Periodo anterior
  df["FLAG_INTERRUPCOES_SET_LAST_YEAR"] = df.groupby(["mes", "DscConjUndConsumidoras"])["FLAG_INTERRUPCOES"].shift(1)
  # Periodo de 2 meses anteriores
  # Test drop shift 2
  # df["FLAG_INTERRUPCOE_SHIFT_2"] = df.groupby(["mes", "DscConjUndConsumidoras"])["FLAG_INTERRUPCOES"].shift(1)

  # Order values by month to check
  df = df.sort_values(['mes'])
  df = df.drop(columns = 'year')
  df = df.drop(columns = 'mes')

  df = df.set_index(["year_month",	"DscConjUndConsumidoras"])

  return df

def year_seasons(df):
  # Estacao do ano. Inclui a estacao do ano. As estacoes comecam no meio do mes, dessa forma em agregacao diaria eh possivel incluir essa consideracao. porem
  # em agregacao foi considerado a estacao no mes e, que a maior quantidade de dias esteve presente.
  df["VERAO"] = np.where(
      df.index.get_level_values(0).str[4:].astype(int).isin([1, 2, 3]),
      1,
      0
  )

  df["OUTONO"] = np.where(
      df.index.get_level_values(0).str[4:].astype(int).isin([4, 5, 6]),
      1,
      0
  )

  df["INVERNO"] = np.where(
      df.index.get_level_values(0).str[4:].astype(int).isin([7, 8, 9]),
      1,
      0
  )


  df["PRIMAVERA"] = np.where(
      df.index.get_level_values(0).str[4:].astype(int).isin([10, 11, 12]),
      1,
      0
  )
  return df


def norm_data(df):
  # Realiza a normalizacao dos dados para as colunas numericas - Para o modelo eh necessario normalizar (colocar as variaveis entre 0 e 1) porque pode ser que uma classe tenha
  # mais impacto que outra
  # O tratamento eh aplicado para todo o conjunto porque todo o conjunto ja eh numerico
  from sklearn.preprocessing import MinMaxScaler
  scaler = MinMaxScaler()

  features = [col for col in df.columns if col != label]

  # Scale only the features because the scale of the output dont impact the convergence
  scaler.fit(df[features])
  df[features] = scaler.transform(df[features])
  return df

"""# Modeling

Modeling is the part where is construct the agorithm to model the process. And is checked the fit.
"""

# Leitura do conjunto de dados para o modelo
data_paths = list(dir_path.glob("Ordens_servico/base_2*"))

df = pd.concat([pd.read_excel(path) for path in data_paths])

df.sample(5)

"""## 1 - Analise descritiva"""

# Total of outages
df.FLAG_INTERRUPCOES.sum()

df[df.UF == "SE"].DscConjUndConsumidoras.unique()

# In some cases, the result of service orders was coming null (this is a check)
df[df.FLAG_INTERRUPCOES.isnull()]

# Amount Units
df.DscConjUndConsumidoras.unique().shape[0]

# Median and Interruptions - its checked the median, mean std and quantile of the distribuition
print(df.FLAG_INTERRUPCOES.describe())

# Histogram of the values
fig, ax = plt.subplots()

ax.tick_params(axis='x', labelrotation = 90)
ax.set_ylabel('Volume de Ocorrência')
ax.set_xlabel('Quantidade de interrupções')
df.FLAG_INTERRUPCOES.hist(bins=200, color='gray')

# Note 1. Due to the high error of CPFL, check the dispertion only for this utility

cpfl_df = df[df.DscConjUndConsumidoras.isin(test_set_json_cpfl['conjuntos'])]

# Histogram of the values
fig, ax = plt.subplots()

ax.tick_params(axis='x', labelrotation = 90)
ax.set_ylabel('Volume de Ocorrência')
ax.set_xlabel('Quantidade de interrupções')
cpfl_df.FLAG_INTERRUPCOES.hist(bins=200, color='gray')

# Median and Interruptions - its checked the median, mean std and quantile of the distribuition
print(cpfl_df.FLAG_INTERRUPCOES.describe())

# Amount Units per State
df_uf_unit_cons_count = pd.DataFrame(df.groupby("UF")["DscConjUndConsumidoras"].count().sort_values(ascending=False)).rename(columns={"DscConjUndConsumidoras": "quantidade_un_cons"})
df_uf_unit_cons_count["total"] = df_uf_unit_cons_count.quantidade_un_cons.sum()
# df_uf_unit_cons_count["percentage"] = df_uf_unit_cons_count
df_uf_unit_cons_count["percentage"] = (df_uf_unit_cons_count["quantidade_un_cons"] / df_uf_unit_cons_count["total"])*100
df_uf_unit_cons_count

print(df.FLAG_INTERRUPCOES.kurtosis())

df.DscConjUndConsumidoras.unique()[0:6]

# Order by year_month
df = df.sort_values(by="year_month")

# Time serie of the values with the examples
conjs = df.DscConjUndConsumidoras.unique()[250:264]
fig, ax = plt.subplots(figsize=(6,2))
ax.tick_params(axis='x', labelrotation = 90)
ax.set_ylabel('Qtd de interrupções por \n Conjunto', fontsize =9)
ax.set_xlabel('Ano-Mes', fontsize = 9)
ax.xaxis.set_tick_params(labelsize=8)
ax.yaxis.set_tick_params(labelsize=8)

print('Conuntos plotados', conjs)

ax.plot(df[df.DscConjUndConsumidoras == conjs[0]]["year_month"], df[df.DscConjUndConsumidoras == conjs[0]]["FLAG_INTERRUPCOES"], color='grey')
ax.plot(df[df.DscConjUndConsumidoras == conjs[1]]["year_month"], df[df.DscConjUndConsumidoras == conjs[1]]["FLAG_INTERRUPCOES"], color='grey', linestyle='dashed')
ax.plot(df[df.DscConjUndConsumidoras == conjs[2]]["year_month"], df[df.DscConjUndConsumidoras == conjs[2]]["FLAG_INTERRUPCOES"], color='grey', linestyle='dashdot')
ax.plot(df[df.DscConjUndConsumidoras == conjs[3]]["year_month"], df[df.DscConjUndConsumidoras == conjs[3]]["FLAG_INTERRUPCOES"], color='grey', linestyle='dotted')


ax.legend(conjs, fontsize =8)

# Teste t de student
mean = df['FLAG_INTERRUPCOES'].mean()

means = []

for i in range(1000):
  means.append(df['FLAG_INTERRUPCOES'].sample(50).mean())

# Muitas regioes apresentam quantidades altas de Interrupcoes
df.FLAG_INTERRUPCOES.plot.box()

# Log transform cause the serie have long tail
# https://www.itl.nist.gov/div898/handbook/eda/section3/eda33e6.htm
df['FLAG_INTERRUPCOES_lognorm'] = np.log10(df['FLAG_INTERRUPCOES'])
df['FLAG_INTERRUPCOES_lognorm'].hist(bins=100)

# Check the quantity of sets que estao sendo considerados para o modelo (ou seja apresentam as informacoes coletadas)
len(df.DscConjUndConsumidoras.unique())

# Check if the time serie have some tendency that can be

# Scatter plot of variables
cols = df.select_dtypes(np.number).columns
cols = [c for c in cols if c not in ["FLAG_INTERRUPCOES"]]


for i, f in enumerate(cols):
  fig, ax = plt.subplots(figsize=(7, 5))
  ax.scatter(df[f], df["FLAG_INTERRUPCOES"], color='grey') # s=5 parameter in ax.scatter() controls the size of the bubbles
  ax.set_xlabel("Quantidade de interrupções")
  ax.set_ylabel(f)

# Plot da quantidade de interrupcoes por conjunto no Brasil

# Coleta 20 conjuntos aleatoriamente no Dataframe
conjuntos = np.random.choice(df.DscConjUndConsumidoras.unique(), size=1)

# Plota algumas serias para verificar o comportamento
for conjunto in conjuntos:
  fig, ax = plt.subplots(figsize=(20,5))

  # Title
  ax.set_title("plot da serie do conjunto {}".format(conjunto))
  # Mean
  mean = np.mean(df[df.DscConjUndConsumidoras == conjunto]["FLAG_INTERRUPCOES"])
  # Plot Serie
  ax.plot(df[df.DscConjUndConsumidoras == conjunto]["year_month"], df[df.DscConjUndConsumidoras == conjunto]["FLAG_INTERRUPCOES"])
  # Plot Mean
  df_size = len(df[df.DscConjUndConsumidoras == conjunto]["year_month"])
  ax.plot(df[df.DscConjUndConsumidoras == conjunto]["year_month"], [mean]*df_size, color='gray')

# Analise de quanto cada coluna tem de Correlacao linear com a quantidade de Interrupcoes

for feature in df.select_dtypes(include=np.number).columns:
  fig, ax = plt.subplots()
  ax.set_title(feature)
  plt.scatter(df[feature], df.FLAG_INTERRUPCOES, axes=ax)

# Plota a matrix de correlacao entre os atribuostos da base de dados. Apos salva em excel para a dissertacao.
df.corr().to_excel(base_path + "/Ordens_servico/matrix_de_correlacao.xlsx")

import geopandas as gpd
gdf = gpd.read_file('https://raw.githubusercontent.com/codeforgermany/click_that_hood/main/public/data/brazil-states.geojson', driver='GeoJSON')
gdf.head(1)

# Faz o plot do mapa de calor de religamentos por cada estado - obs. minas nao esta aparecendo na analise (verificar)

colorscale = ["#f7fbff","#ebf3fb","#deebf7","#d2e3f3","#c6dbef","#b3d2e9","#9ecae1",
              "#85bcdb","#6baed6","#57a0ce","#4292c6","#3082be","#2171b5","#1361a9",
              "#08519c","#0b4083","#08306b"]

states_df = df.groupby("UF")["FLAG_INTERRUPCOES"].sum().reset_index()
# save the dataset for external plot
states_df.to_excel(base_path + 'Ordens_servico/states_int_amount.xlsx')
fig_cloropleth = px.choropleth(states_df, geojson=counties, locations='UF', scope='south america', hover_name ="FLAG_INTERRUPCOES", labels={'FLAG_INTERRUPCOES':'FLAG_INTERRUPCOES'}, featureidkey="properties.sigla", color_continuous_scale="gray_r",
                               color="FLAG_INTERRUPCOES",  width = 1000 , height=1000,
)
fig_cloropleth.layout.coloraxis.colorbar.len = 0.6
fig_cloropleth.layout.coloraxis.colorbar.title = 'Quantidade Interrupcoes'
fig_cloropleth.layout.coloraxis.colorbar.titlefont.size = 15
fig_cloropleth.update_layout(
    title_text = 'Quantidade de Interrupcoes por estado do Brasil - 2020, 2021 e 2022 - ANEEL',
)

fig_cloropleth.show()
print("Media de Religamentos nos estados:", np.mean(states_df))
print("Desvio Padrao de Religamentos nos estados:", np.std(states_df))

states_df.sort_values(by='FLAG_INTERRUPCOES', ascending= False)

# Filtra apenas uma amostra dos dados para fazer os testes.
# df = df[df.uf == "RJ"]
# # Apenas para o Rio de Janeiro no conjunto tem-se 1066357 interrupcoes e 22 municios
# df.FLAG_INTERRUPCOES.sum(), len(df.NOME_MUNICIPIO.unique())

"""Correlation - Check the dispertion of each variable with the target"""

# Scatter only is defined with numerical cols
num_cols = df.select_dtypes(np.number).columns

for col in num_cols:
  fig, ax = plt.subplots()
  ax.scatter(df[col], df.FLAG_INTERRUPCOES)
  ax.tick_params(axis='x', labelrotation = 90)
  ax.set_ylabel(col)
  ax.set_xlabel('Quantidade de interrupções')
  ax.scatter(df[col], df.FLAG_INTERRUPCOES, color='gray')

"""## 2 - Preprocess

Para o modelo entender os dados passados, eh necessario transformar as variaveis categoricas em numericas. Ja as variaveis numericas pode ser necessarios algum tratamento (como normalizacao, ou padronizacao).

### Retira os outliers
"""

# Plota o histograma de interrupcoes para ver
df.FLAG_INTERRUPCOES.hist(bins=100)

plt.boxplot(df.FLAG_INTERRUPCOES)

df.FLAG_INTERRUPCOES.quantile(0.1)

df = treat_outliers(df)

df.FLAG_INTERRUPCOES.hist(bins=100)

"""### Tratamento dos Valores Nulos"""

# Algumas colunas parecem apresentar valores nulos. O que pode ser um problema para o modelo
df.info()

# Parece que a coluna referente a regiao metropolitana tem dados faltantes quando o municipio nao eh caracterizado desta forma. Essa informacao eh importante porque uma Região metropolitana eh composta por
# um nucleo urbano densamente povoado.
df[df["Região Metropolitana"].notnull()]

# Parece pelos dados que o municipios com baixa concentracao urbana estao sem classificacao. Por hora vamos marcar como nao classificados
df[df["Tipo Concentração Urbana"].notnull()]["Tipo Concentração Urbana"].unique()

# Faz a verificacao de registros nulos nos dados de vegetacao
df[df["veg_class"].isnull()]["veg_class"].shape[0]

df = fill_null(df)

"""### Inclui as variaveis de clusters"""

df = add_clusters_info(df)

df.shape

# 1256 registros de clusters sao nulos
df[df.labels.isnull()].shape

# Completa os registros com labels de cluster vazios com uma nova classe

df.columns

"""Define as caracteristicas do modelo"""

df = select_features(df, index, cols)

df.head()

df = drop_duplic(df)

# Vamos analisar os dados outliers. Dessa forma, vamos plotar o histograma (barras em variaveis de categorias) de todas as variaveis do conjunto. E enteder se tem algum conjunto fora do padrao esperado
# columns = df.columns

# for col in columns:
#   fig, ax = plt.subplots(figsize=(20, 4))
#   fig.suptitle("{} histogram".format(col), fontsize=10)
#   ax.hist(df[col].values)
#   plt.show()
#   plt.close()

# Populacao estimada - Populacao estimada parece apresentar alguns casos acima do padrao, porem pelo que analisado parece ser um municipio grande (Ex. Rio de Janeiro), nao justifica neste caso a retirada de outliers
# Max preciptacao - A precipitacao maxima possui grande parte dos valores na faixa de 0 a 60, porem alguns poucas casos (em relacao a base de dados) apresentam valores proximos de 100.
# para esse caso foi decidido nao retirar esses registros por nao ser um valor altamente discrepante
# Flag interrucoes -  A flag que marca o output do modelo - quantidade de interrupcoes -
# apresenta alguns casos acima do padrao, porem pelo que analisado parece ser um municipio grande (Ex. Rio de Janeiro), nao se justifica neste caso a retirada de outliers

df

df[df[label] == df[label].max()][label]

# Teste - Passa o mes para variavel categorica
def transform_month(df):
  df["mes"] = df["mes"].astype('str')
  return df

df = transform_month(df)

cat_cols = df.select_dtypes(include=['object']).columns
cat_cols

# Vamos verificar o numero de possiveis valores de serem encontrados em cada categoria
for col in df[cat_cols]:
  print(col, len(df[col].unique()))

df = one_hot_econding(df)

df

df =   fill_null_mean_tratment(df, integer_null_cols_dict)

"""### Cria variáveis de Series Temporais

- Variaveis de lag (quantidade de interrupcoes em meses passados) - somente eh criado para com a variavel dependente
"""

df = time_shift(df)

df = year_seasons(df)

# Retira as colunas dos dois primeiros meses por conterem valores nulos apos a inclusao do lag
# last_month = df.reset_index().year_month.astype(int).min()
# second_last_month = last_month + 1

# df = df[df.index.get_level_values(0).astype(int) > second_last_month]

df

"""### Normaliza os dados das caracteristica

Normalizacao utilizada: min, max
"""

df = norm_data(df)

df

# Persiste o conjunto de dados tratado
df.to_excel(base_path + "/Ordens_servico/preprocess.xlsx")

# Persiste a base preprocessada dividida em 4 modelos a partir dos clusters definidos
# clusters = [c for c in df.columns if 'label' in c]

# for cluster in clusters:
#   df[df[cluster] == 1].to_excel(base_path + "/Ordens_servico/preprocess_cluster_{}.xlsx".format(cluster))

"""# 3- Modelagem - Aplicacao dos Algoritmos

Apos ser realizado tratamento dos dados e a coodificacao das variaveis para a entrada no modelo. Eh realizada aplicacao dos algoritmos. Tendo como objetivo realizar a previsao da quantidade de ordens de servico no mes/municipio (ou agregacao territorial - obs. atualmente o modelo esta por municipio, porem eh possivel outras agregacoes, como o Conjunto)
"""

modeling_df = pd.read_excel(base_path + "Ordens_servico/preprocess.xlsx", index_col=[0,	1])
# Le os conjuntos de cada clusters
modeling_clusters_df = []
clusters = [c for c in modeling_df.columns if 'label' in c]
for cluster in clusters:
  modeling_clusters_df.append(pd.read_excel(base_path + "/Ordens_servico/preprocess_cluster_{}.xlsx".format(cluster), index_col=[0,	1]))

# For Energisa sergipe validation the model performance without the clusters variables is better
# modeling_df = modeling_df.drop(columns=clusters)

modeling_df

# Calculate the MAPE error
def MAPE(y_true, y_ext):
  "Calculater MAPE error"
  value = ( np.sum(np.absolute(y_true - y_ext)/ y_true) / len(y_true) )*100
  return value

# Calculate the SSE error
def MSE(y_true, y_ext):
  "Calculater MAPE error"
  value = ( np.sum(np.absolute(y_true - y_ext)**2) / len(y_true) )
  return value

"""## 3.1 - Baseline

Para verificar a qualidade dos demais modelo, é sabido que é necessário ter um modelo comparativo simples. Dessa forma, é criado um modelo base. Com isso, inicialmente vamos testar um modelo simples e verificar se melhora a métrica de Previsão baseline sobre a media. O primeiro modelo que vamos utilizar é uma previsão de Naive Sazonal.
"""

# Create a baseline metrics, which the values for the next year are the last one
def last_year_interruptions(modeling_df):
  modeling_df = modeling_df.reset_index()

  modeling_df["mes"] = modeling_df.year_month.astype('str').str[4 :].astype('int')
  modeling_df["year"] = modeling_df.year_month.astype('str').str[0: 4].astype('int')

  modeling_df = modeling_df.sort_values(['year', 'mes'])
  modeling_df["FLAG_INTERRUPCOES_SET_LAST_YEAR"] = modeling_df.groupby(["mes", "DscConjUndConsumidoras"])["FLAG_INTERRUPCOES"].shift(1)

  # Order values by month to check
  modeling_df = modeling_df.sort_values(['mes'])
  modeling_df = modeling_df.drop(columns = 'year')
  modeling_df = modeling_df.drop(columns = 'mes')

  modeling_df = modeling_df.set_index(["year_month",	"DscConjUndConsumidoras"])

  return modeling_df

modeling_df = last_year_interruptions(modeling_df)

for ind, cluster_df in enumerate(modeling_clusters_df):
  modeling_clusters_df[ind] = last_year_interruptions(cluster_df)

def baseline(modeling_df, separe_type='general'):
  # Baseline for Naive Sazonal Method

  # Baseline 1.1
  if separe_type == "general":
    # Check the result for the two last months in 2022
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int').isin([202211, 202212])]
    title = "Quantidade de casos de interrupções real vs. inferência modelo - nov-dez/2022"


  # Baseline 1.2
  if separe_type == "energisa_se":
    # Check the result for the inference of 2021, 2022 for the sets that are supplied  by Energisa SE
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = test_df[test_df.index.get_level_values(1).isin(test_set_json['conjuntos'])]
    print("Quantidade de Conjuntos Energisa SE", len(np.unique(test_set_json['conjuntos'])))
    q_sets = len(np.unique(test_set_json['conjuntos']))
    title = "Quantidade de casos de interrupções real vs. inferência \n nov-dez/2022 - {} Conjuntos - Resultado do Baseline".format(q_sets)

  # Baseline 1.3
  if separe_type == "light":
  # Check the result for the inference of 2021, 2022 for the sets that are supplied  by Light
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = test_df[test_df.index.get_level_values(1).isin(test_set_json_light['conjuntos'])]
    print("Quantidade de Conjuntos Light", len(np.unique(test_set_json_light['conjuntos'])))
    q_sets = len(np.unique(test_set_json_light['conjuntos']))
    title = "Quantidade de casos de interrupções real vs. inferência \n nov-dez/2022 - {} Conjuntos - Resultado do Baseline".format(q_sets)

  # Baseline 1.4
  if separe_type == "cemig":
  # Check the result for the inference of 2021, 2022 for the sets that are supplied  by Cemig
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = test_df[test_df.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])]
    print("Quantidade de Conjuntos cemig", len(np.unique(test_set_json_cemig['conjuntos'])))
    q_sets = len(np.unique(test_set_json_cemig['conjuntos']))
    title = "Quantidade de casos de interrupções real vs. inferência \n nov-dez/2022 - {} Conjuntos - Resultado do Baseline".format(q_sets)

  # Baseline 1.5
  if separe_type == "copel":
  # Check the result for the inference of 2021, 2022 for the sets that are supplied  by Cemig
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = test_df[test_df.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])]
    print("Quantidade de Conjuntos copel", len(np.unique(test_set_json_copel['conjuntos'])))
    q_sets = len(np.unique(test_set_json_copel['conjuntos']))
    title = "Quantidade de casos de interrupções real vs. inferência \n nov-dez/2022 - {} Conjuntos - Resultado do Baseline".format(q_sets)

  # Baseline 1.6
  if separe_type == "cpfl":
  # Check the result for the inference of 2021, 2022 for the sets that are supplied  by Cemig
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202200]
    test_df = test_df[test_df.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])]
    print("Quantidade de Conjuntos cpfl", len(np.unique(test_set_json_cpfl['conjuntos'])))
    q_sets = len(np.unique(test_set_json_cpfl['conjuntos']))
    title = "Quantidade de casos de interrupções real vs. inferência \n nov-dez/2022 - {} Conjuntos - Resultado do Baseline".format(q_sets)



  # Some months are without data to 2021, so dont have data to 2022, therefore its needed to drop this cases
  test_df = test_df.dropna()


  fig, ax = plt.subplots()
  title = "Valores Previstos vs. Observados - Baseline"
  ax.scatter(test_df["FLAG_INTERRUPCOES"], test_df["FLAG_INTERRUPCOES_SET_LAST_YEAR"], s=15,color='black')
  ax.set_title(title, loc='left',fontsize=10)
  ax.tick_params(axis='x', labelrotation = 90)
  ax.set_xlabel('Interrupcoes Reais')
  ax.set_ylabel('Interrupcoes Previstas')
  # Plot a diagonal line
  ax.plot([0, 1], [0, 1], transform=ax.transAxes, color='grey')



  # R2 and MAPE to the baseline
  print("baseline metrics R2, MAPE and MSE",
        r2_score(test_df["FLAG_INTERRUPCOES"], test_df["FLAG_INTERRUPCOES_SET_LAST_YEAR"]),
        MAPE(test_df["FLAG_INTERRUPCOES"], test_df["FLAG_INTERRUPCOES_SET_LAST_YEAR"]),
        MSE(test_df["FLAG_INTERRUPCOES"], test_df["FLAG_INTERRUPCOES_SET_LAST_YEAR"])
        )

"""## 3.2 - Regressão Linear

Para ter uma comparacao inicial vamos verificar conseguimos solucionar o problema utilizando um algoritmo de regressão linear, sendo este o primeiro modelo que iremos testar. A sua forma é tal que:



$
V_{1}C_{1} + V_{2}C_{2} + V_{3}C_{3} + V_{4}C_{4} ... V_{n}C_{n} = y
$

Sendo V as caracteristicas de relação do problema e C referente aos coeficientes da Regreessao Linear.
"""

# https://www.hindawi.com/journals/tswj/2014/509429/ -  Techiniques
# http://pzs.dstu.dp.ua/DataMining/mls/bibl/Nonlinear%20regression.pdf - Nonlinear Regression
# https://omyllymaki.medium.com/gauss-newton-algorithm-implementation-from-scratch-55ebe56aac2e - gauss newton algorithm
# https://cs.stanford.edu/~ermon/cs325/slides/ml_nonlin_reg.pdf - non linear models - Stanford edu
# https://pieriantraining.com/nonlinear-regression-in-machine-learning-python/ - Nonlinear Regression with sklearn in python

label = "FLAG_INTERRUPCOES"

# dropna
# Comment cause the column interruption in last year have nulls to 2021, check how to solve
# modeling_df = modeling_df.dropna()

modeling_df

# Separe the base in train / test

def sep_train_test(modeling_df, separe_type='general', testyear='2023'):
  # Utilizando um algoritmo linear sklearn - https://medium.com/datadenys/using-polynomial-linear-regression-for-nonlinear-data-with-python-scikit-8ccfc235edc9
  label = "FLAG_INTERRUPCOES"

  # Its a time series, so for prediction we have to order the data by date (we will include districty too)
  modeling_df = modeling_df.sort_values(["year_month", "DscConjUndConsumidoras"])

  # Because we are using the information of the last year outages in the model, have to exclude the null values of the first year (2020)
  modeling_df = modeling_df[modeling_df.index.get_level_values(0).astype('int') > 202100]

  # Drop null values
  modeling_df = modeling_df.dropna()

  X = modeling_df.drop(columns=label)
  Y = modeling_df[label]


  # Divisoes da base de treinamento e teste
  # 1. 20% teste
  # X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101, shuffle=False)


  # 2. Ultimos 2 meses para o teste
  if separe_type ==  'general':
    max_value = 202212  # X.reset_index().year_month.max()
    second_max_value = 202211 # X[X.index.get_level_values(0) != max_value].reset_index().year_month.max()
    test_filter = [x in [max_value, second_max_value] for x in X.index.get_level_values(0)]


    X_train = X[[not x for x in test_filter]  ]
    X_test = X[test_filter]
    y_train = Y[ [not x for x in test_filter]  ]
    y_test = Y[test_filter]

  # 3. Base de teste pronta SE / 2022
  if separe_type ==  'energisa_se':
    X_train = X[~X.index.get_level_values(1).isin(test_set_json['conjuntos'])]
    X_test = X[(X.index.get_level_values(1).isin(test_set_json['conjuntos'])) & (X.index.get_level_values(0).astype('str').str.contains(testyear))]
    y_train = Y[~Y.index.get_level_values(1).isin(test_set_json['conjuntos'])]
    y_test = Y[(Y.index.get_level_values(1).isin(test_set_json['conjuntos'])) & (Y.index.get_level_values(0).astype('str').str.contains(testyear))]


  if separe_type == 'light':
    X_train = X[~X.index.get_level_values(1).isin(test_set_json_light['conjuntos'])]
    X_test = X[(X.index.get_level_values(1).isin(test_set_json_light['conjuntos'])) & (X.index.get_level_values(0).astype('str').str.contains(testyear))]
    y_train = Y[~Y.index.get_level_values(1).isin(test_set_json_light['conjuntos'])]
    y_test = Y[(Y.index.get_level_values(1).isin(test_set_json_light['conjuntos'])) & (Y.index.get_level_values(0).astype('str').str.contains(testyear))]

  if separe_type == 'cemig':
    X_train = X[~X.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])]
    X_test = X[(X.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])) & (X.index.get_level_values(0).astype('str').str.contains(testyear))]
    y_train = Y[~Y.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])]
    y_test = Y[(Y.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])) & (Y.index.get_level_values(0).astype('str').str.contains(testyear))]

  if separe_type == 'copel':
    X_train = X[~X.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])]
    X_test = X[(X.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])) & (X.index.get_level_values(0).astype('str').str.contains(testyear))]
    y_train = Y[~Y.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])]
    y_test = Y[(Y.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])) & (Y.index.get_level_values(0).astype('str').str.contains(testyear))]

  if separe_type == 'cpfl':
    X_train = X[~X.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])]
    X_test = X[(X.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])) & (X.index.get_level_values(0).astype('str').str.contains(testyear))]
    y_train = Y[~Y.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])]
    y_test = Y[(Y.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])) & (Y.index.get_level_values(0).astype('str').str.contains(testyear))]


  # Check the shape of train and test and the sets
  print("Qtd de Registros de Treino", "Qtd de Registros de Teste", X_train.shape[0], X_test.shape[0])
  print("Qtd de Conjuntos de Treino", "Qtd de Conjuntos de Teste", X_train.index.get_level_values(1).unique().shape[0], X_test.index.get_level_values(1).unique().shape[0])

  # Model
  # Dropnull from train
  train = pd.concat([X_train, y_train], axis=1)
  train = train.dropna()
  X_train = train.drop(columns=label)
  y_train = train[label]

  model = LinearRegression(fit_intercept=True)
  model.fit(X_train, y_train)
  return model, X_train, X_test, y_train, y_test

def inference(X_test, y_test, prefix='', simulation=False):
  y_test = pd.DataFrame(y_test)
  y_test["y_p"] = model.predict(X_test)

  if not simulation:
    real_predict_plot(y_test, prefix)
    residuals_plot(y_test, prefix)
    residuals_boxplot(y_test, prefix)
    resuduals_time_serie(y_test, prefix='')
    # r2 score and MAPE
    print("R2", r2_score(y_test[label], y_test["y_p"]),  "MAPE", MAPE(y_test[label], y_test.y_p), "MSE", MSE(y_test[label], y_test.y_p))

  return y_test

def real_predict_plot(y_test, prefix=''):
  fig, ax = plt.subplots()
  q_sets = len(np.unique(test_set_json['conjuntos']))
  if prefix == '':
    title = "Valores Previstos vs. Observados"
  else:
    title = "Valores Previstos vs. Observados - {}".format(prefix)

  ax.scatter(y_test[label], y_test["y_p"], s=15,color='black')
  ax.set_title(title, loc='left',fontsize=10)
  ax.tick_params(axis='x', labelrotation = 90)
  ax.set_xlabel('Interrupcoes Reais')
  ax.set_ylabel('Interrupcoes Previstas')
  # Plot a diagonal line
  ax.plot([0, 1], [0, 1], transform=ax.transAxes, color='grey')

def residuals_plot(y_test, prefix=''):
  # Histrogram of residuals
  fig, ax = plt.subplots()
  y_test["error"] = y_test["y_p"] - y_test[label]

  ax.hist(y_test.error, bins=100, color="k", alpha=0.8)

  if prefix == '':
    title = "Histograma dos Resíduos de Previsão"
  else:
    title = "Histograma dos Resíduos de Previsão - {}".format(prefix)

  ax.set_title(title, loc='left',fontsize=10)
  ax.tick_params(axis='x', labelrotation = 90)
  ax.set_xlabel('Resíduo')
  ax.set_ylabel('')
  print("Assimetria - Skew:", y_test.error.skew())

# Plot error Dispersion
def residuals_boxplot(y_test, prefix=''):
  fig, ax = plt.subplots()

  y_test["error"] = y_test.FLAG_INTERRUPCOES - y_test.y_p
  y_test["percent_error"] = np.abs(y_test.FLAG_INTERRUPCOES - y_test.y_p)*100 / y_test.FLAG_INTERRUPCOES

  ax.boxplot(y_test["percent_error"])

  # ax.set_title(title, loc='left',fontsize=10)
  ax.tick_params(axis='x', labelrotation = 90)

  ax.set_ylabel("Variação Percentual do Real vs. a Previsão")
  print(y_test["percent_error"].median())

# Plot the time serie of the residuals
def resuduals_time_serie(y_test, prefix=''):
  fig, ax = plt.subplots()

  time_sr_df = pd.DataFrame(y_test.reset_index().groupby('year_month')['error'].mean()).rename(columns={'error': 'mean_error'})
  months = time_sr_df.shape[0]

  ax.plot(np.arange(1, months + 1), time_sr_df.mean_error, color='black')

  if prefix == '':
    title = "Serie temporal dos residuos de previsão"
  else:
    title = "Serie temporal dos residuos de previsão - {}".format(prefix)

  ax.set_title(title, loc='left',fontsize=10)
  ax.tick_params(axis='x', labelrotation = 90)
  ax.set_xlabel('Mês')
  ax.set_ylabel('Resíduo')

"""## 3.2.1 - Aplica o modelo a nível Nacional"""

baseline(modeling_df)
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df)
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test, "Modelo Nivel Nacional")

# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]

"""## 3.2.1 - Aplica o Modelo para Energisa Sergipe"""

baseline(modeling_df, separe_type='energisa_se')
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='energisa_se')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test, "Energisa Sergipe")

"""## 3.2.1 Aplica o modelo para Distribuidora Light"""

baseline(modeling_df, separe_type='light')
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='light')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test)

y_test.sort_values(by='percent_error', ascending=False).head(60)

"""## 3.2.1 Aplica o modelo para Distribuidora Cemig"""

baseline(modeling_df, separe_type='cemig')
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='cemig')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test, "Cemig")

y_test.sort_values(by='percent_error', ascending=False).head(20)

"""## 3.2.1 Aplica o modelo para Distribuidora Copel"""

baseline(modeling_df, separe_type='copel')
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='copel')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test, "Copel")

"""## 3.2.1 Aplica o modelo para Distribuidora cpfl"""

baseline(modeling_df, separe_type='cpfl')
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='cpfl')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test, "CPFL")

"""## 3.2.1 - Aplica o modelo para o conjunto dividido por clusters"""

for cluster_df in modeling_clusters_df:

  try:
    model, X_train, X_test, y_train, y_test = sep_train_test(cluster_df)
    y_test = inference(X_test, y_test)
    real_predict_plot(y_test)
  except:
    pass
    print("train fail")

"""### 3.2.1 -  Estatistica F - Teste de hipotese realizado para o modelo

O teste F é um teste estatística que avalia a variancia entre distribuições. Ele pode ser utilizada em diversas aplicações. E uma delas é o teste se um modelo se ajusta aos dados.
"""

from scipy.stats import ttest_ind

"""O teste é aplicado em conjunto do o teste realizado com a estatística T. Foi escolhida esta forma de aplicação dado que a biblioteca utilizada faz a geração de um relatório completo que contém tanto a estatística T, quanto F.

### 3.2.2 - Selecao de Variaveis - Test T de Student to analyze the p-value to each variable
Após aplicada a estatistica F é então aplicado o Teste T para verificar a relevancia de cada uma das variaveis para o modelo.

É importante que, após a aplicado do teste T, seja recriado o modelo com as variáveis que foram selecionadas e avaliada a estatística F novamente. Este processo de aplicação da estatística T e F é feito recursivamente até que não se tenha nenhuma variável a ser retirada no teste T.

### Modelo a Nível Nacional
"""

number_drop_variables = None
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df)
selected_variables = X_train.columns

while number_drop_variables != 0:

  X = X_train[selected_variables]
  y = y_train

  X2 = sm.add_constant(X)
  est = sm.OLS(y, X2)
  est2 = est.fit()
  print(est2.summary())

  ols_sumary = pd.DataFrame(est2.summary().tables[1].data[1:], columns=est2.summary().tables[1].data[0])
  ols_sumary["P>|t|"] = ols_sumary["P>|t|"].astype('float')
  print("Quantidade de variaveis antes e apos a selecao")
  print(ols_sumary.shape[0] - 1, ols_sumary[ols_sumary["P>|t|"] < 0.05].shape[0] - 1)
  selected_variables = ols_sumary[ols_sumary["P>|t|"] < 0.05][''].iloc[1:].values
  selected_variables

  # Create a new variable containing the number of variables that was droped in the feature selection
  number_drop_variables = len(X.columns) - len(selected_variables)
  print("quantidade de variaveis retirados do estimador", number_drop_variables)

print("Lista de variaveis selecionadas")
for attr in selected_variables:
  print(attr)

baseline(modeling_df)
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df[np.append(selected_variables, label)])
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test)

"""### Energisa SE"""

number_drop_variables = None
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='energisa_se')
selected_variables = X_train.columns

while number_drop_variables != 0:

  X = X_train[selected_variables]
  y = y_train

  X2 = sm.add_constant(X)
  est = sm.OLS(y, X2)
  est2 = est.fit()
  print(est2.summary())

  ols_sumary = pd.DataFrame(est2.summary().tables[1].data[1:], columns=est2.summary().tables[1].data[0])
  ols_sumary["P>|t|"] = ols_sumary["P>|t|"].astype('float')
  print("Quantidade de variaveis antes e apos a selecao")
  print(ols_sumary.shape[0] - 1, ols_sumary[ols_sumary["P>|t|"] < 0.05].shape[0] - 1)
  selected_variables = ols_sumary[ols_sumary["P>|t|"] < 0.05][''].iloc[1:].values
  selected_variables

  # Create a new variable containing the number of variables that was droped in the feature selection
  number_drop_variables = len(X.columns) - len(selected_variables)
  print("quantidade de variaveis retirados do estimador", number_drop_variables)

print("Lista de variaveis selecionadas")
for attr in selected_variables:
  print(attr)
df = pd.DataFrame(selected_variables)
df.to_excel(base_path + '/Ordens_servico/selected_variables_energisa_se.xlsx')

model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df[np.append(selected_variables, label)], separe_type='energisa_se')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test)

"""### Cemig"""

number_drop_variables = None
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='cemig')
selected_variables = X_train.columns

while number_drop_variables != 0:

  X = X_train[selected_variables]
  y = y_train

  X2 = sm.add_constant(X)
  est = sm.OLS(y, X2)
  est2 = est.fit()
  print(est2.summary())

  ols_sumary = pd.DataFrame(est2.summary().tables[1].data[1:], columns=est2.summary().tables[1].data[0])
  ols_sumary["P>|t|"] = ols_sumary["P>|t|"].astype('float')
  print("Quantidade de variaveis antes e apos a selecao")
  print(ols_sumary.shape[0] - 1, ols_sumary[ols_sumary["P>|t|"] < 0.05].shape[0] -1)
  selected_variables = ols_sumary[ols_sumary["P>|t|"] < 0.05][''].iloc[1:].values
  selected_variables

  # Create a new variable containing the number of variables that was droped in the feature selection
  number_drop_variables = len(X.columns) - len(selected_variables)
  print("quantidade de variaveis retirados do estimador", number_drop_variables)

print("Lista de variaveis selecionadas")
for attr in selected_variables:
  print(attr)
df = pd.DataFrame(selected_variables)
df.to_excel(base_path + '/Ordens_servico/selected_variables_cemig.xlsx')

model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df[np.append(selected_variables, label)], separe_type='cemig')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test)

"""### Copel"""

number_drop_variables = None
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='copel')
selected_variables = X_train.columns

while number_drop_variables != 0:

  X = X_train[selected_variables]
  y = y_train

  X2 = sm.add_constant(X)
  est = sm.OLS(y, X2)
  est2 = est.fit()
  print(est2.summary())

  ols_sumary = pd.DataFrame(est2.summary().tables[1].data[1:], columns=est2.summary().tables[1].data[0])
  ols_sumary["P>|t|"] = ols_sumary["P>|t|"].astype('float')
  print("Quantidade de variaveis antes e apos a selecao")
  print(ols_sumary.shape[0] - 1, ols_sumary[ols_sumary["P>|t|"] < 0.05].shape[0] - 1)
  selected_variables = ols_sumary[ols_sumary["P>|t|"] < 0.05][''].iloc[1:].values
  selected_variables

  # Create a new variable containing the number of variables that was droped in the feature selection
  number_drop_variables = len(X.columns) - len(selected_variables)
  print("quantidade de variaveis retirados do estimador", number_drop_variables)

print("Lista de variaveis selecionadas")
for attr in selected_variables:
  print(attr)
df = pd.DataFrame(selected_variables)
df.to_excel(base_path + '/Ordens_servico/selected_variables_copel.xlsx')

model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df[np.append(selected_variables, label)], separe_type='copel')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test)

"""### CPFL"""

number_drop_variables = None
model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df, separe_type='cpfl')
selected_variables = X_train.columns

while number_drop_variables != 0:

  X = X_train[selected_variables]
  y = y_train

  X2 = sm.add_constant(X)
  est = sm.OLS(y, X2)
  est2 = est.fit()
  print(est2.summary())

  ols_sumary = pd.DataFrame(est2.summary().tables[1].data[1:], columns=est2.summary().tables[1].data[0])
  ols_sumary["P>|t|"] = ols_sumary["P>|t|"].astype('float')
  print("Quantidade de variaveis antes e apos a selecao")
  print(ols_sumary.shape[0] - 1, ols_sumary[ols_sumary["P>|t|"] < 0.05].shape[0] - 1)
  selected_variables = ols_sumary[ols_sumary["P>|t|"] < 0.05][''].iloc[1:].values
  selected_variables

  # Create a new variable containing the number of variables that was droped in the feature selection
  number_drop_variables = len(X.columns) - len(selected_variables)
  print("quantidade de variaveis retirados do estimador", number_drop_variables)

print("Lista de variaveis selecionadas")
for attr in selected_variables:
  print(attr)
df = pd.DataFrame(selected_variables)
df.to_excel(base_path + '/Ordens_servico/selected_variables_cpfl.xlsx')

model, X_train, X_test, y_train, y_test = sep_train_test(modeling_df[np.append(selected_variables, label)], separe_type='cpfl')
# Retorna os dados de volta da transformacao logaritmica
# y_test[label] = 10**y_test[label]
# y_test["y_p"] = 10**y_test["y_p"]
y_test = inference(X_test, y_test)

"""### 3.2.3 - Persistência do Resultado"""

hour = datetime.datetime.now().strftime("%Y_%m_%d_%H")

# Verifica a importancia de cada variavel na previsao, o modelo parece estar sobre ajustado pois os resultados foram acima do esperado para
# o modelo de regressao linear

feat_import_dic = {}
# Get feature_name and importance in a dict, and show
for feat, importance in zip(model.feature_names_in_, model.coef_):
  feat_import_dic[feat] =  importance

# Salva o dicionario de features importance em um excel
feat_import_df = pd.DataFrame(data=feat_import_dic, index=[0]).T.reset_index()
feat_import_df.columns = ["Feat", "Coef"]
feat_import_df.to_excel(base_path + "/Ordens_servico/feat_imp_linear_model_{}.xlsx".format(hour))

dict(sorted(feat_import_dic.items(), key=lambda item: item[1]))
# feat_import_dic

# Persiste o modelo e os dados de previsao
# https://scikit-learn.org/stable/model_persistence.html - model persistence in sklearn

import pickle

with open(base_path + "/Ordens_servico/linear_model_{}.pk".format(hour), 'wb') as file:

  pickle.dump(model, file)

test = X_test.join(y_test)
test.to_csv(base_path + "/Ordens_servico/linear_model_test_data_{}.csv".format(hour))

"""### 3.2.4 - Verificacao erro por conjunto da Energisa Sergipe

Apos realizar a estimacao, e ter as informacoes de previsao para os dados de teste. Verfica o resultado para cada conjunto (em separado) da distribuidora Energisa Sergipe
"""

# Include the X_test last year interruptions in the dataset to check the error to this mensure to
y_test["FLAG_INTERRUPCOES_SET_LAST_YEAR"] = X_test["FLAG_INTERRUPCOES_SET_LAST_YEAR"]

conjuntos  = np.unique(test_set_json['conjuntos'])  # Unique sets
sheet_results = {}

for conjunto in conjuntos:
  y_test_c = y_test[y_test.index.get_level_values(1) == conjunto]
  print(conjunto)
  try:
    print(MAPE(y_test_c["FLAG_INTERRUPCOES"], y_test_c["y_p"]))
    print(r2_score(y_test_c["FLAG_INTERRUPCOES"], y_test_c["y_p"]))
    print(MAPE(y_test_c["FLAG_INTERRUPCOES"], y_test_c["FLAG_INTERRUPCOES_SET_LAST_YEAR"]))
    sheet_results[conjunto] = (MAPE(y_test_c["FLAG_INTERRUPCOES"], y_test_c["y_p"]), r2_score(y_test_c["FLAG_INTERRUPCOES"], y_test_c["y_p"]), MAPE(y_test_c["FLAG_INTERRUPCOES"], y_test_c["FLAG_INTERRUPCOES_SET_LAST_YEAR"]))
  except Exception:
    print("Not finded info to set:", conjunto)
    pass

energisa_sergipe_data = pd.DataFrame(sheet_results).T.rename(columns={0: 'MAPE', 1: "R2", 2: "MAPE BASELINE"})

energisa_sergipe_data.to_excel(base_path + "Ordens_servico/output/test_energisa_sergipe.xlsx")

# Check the percentage of time when MAPE is greater then the MAPE BASELINE
energisa_sergipe_data[np.absolute(energisa_sergipe_data.MAPE) < np.absolute(energisa_sergipe_data["MAPE BASELINE"])].shape

1 -(7/27)

conjunto_data = y_test[y_test.index.get_level_values(1) == "ARACAJU"]
plt.scatter(conjunto_data["FLAG_INTERRUPCOES"], conjunto_data["y_p"])

"""### 3.2.5 - Aplicacao de um modelo por Estado

Dado a alta variabilidade do territorio Brasileiro é treinado um modelo por estado e verificado o resultado em relacao ao modelo geral para 2 estados brasileiros, RJ e SE.
"""

modeling_df = pd.read_excel(base_path + "/Ordens_servico/preprocess.xlsx", index_col=[0,	1])
# Le os conjuntos de teste da Energisa Sergipe
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_ENERGISA_SERGIPE.json", 'r') as f:
  test_set_json = json.load(f)
with open("/content/drive/MyDrive/Mestrado/Ordens_servico/Ordens_servico/CONJUNTOS_LIGHT.json", 'r') as f:
  test_set_json_light = json.load(f)

test_set_json_light

modeling_df

# Separe the base in train / test

# Utilizando um algoritmo linear sklearn - https://medium.com/datadenys/using-polynomial-linear-regression-for-nonlinear-data-with-python-scikit-8ccfc235edc9
label = "FLAG_INTERRUPCOES"

# Its a time series, so for prediction we have to order the data by date (we will include districty too)
modeling_df = modeling_df.sort_values(["year_month", "DscConjUndConsumidoras"])


# 1. Base de teste pronta RJ / 2022

# model_df = modeling_df[modeling_df.UF_RJ == 1]

# X = model_df.drop(columns=label)
# Y = model_df[label]

# X_train, y_train, X_text, y_test = train_test_split(test_size=0.3, random_stage=20)

# 2. Base de teste pronta com todos os Estados do Sergipe / 2022

model_df = modeling_df[(modeling_df.UF_SE == 1)]

# 3. Base de teste pronta com todos os Estados do Nordeste / 2022

# model_df = modeling_df[(modeling_df.UF_SE == 1) | (modeling_df.UF_BA == 1) | (modeling_df.UF_MA == 1) | (modeling_df.UF_PI == 1) | (modeling_df.UF_CE == 1) &
#                        (modeling_df.UF_RN == 1) | (modeling_df.UF_PB == 1) | (modeling_df.UF_PE == 1) | (modeling_df.UF_AL == 1)
#                        ]


# Divide the test and train data (Teste com ENERGISA)
# X = model_df.drop(columns=label)
# Y = model_df[label]
# X_train = X[~X.index.get_level_values(1).isin(test_set_json['conjuntos'])]
# X_test = X[X.index.get_level_values(1).isin(test_set_json['conjuntos'])]
# y_train = Y[~Y.index.get_level_values(1).isin(test_set_json['conjuntos'])]
# y_test = Y[Y.index.get_level_values(1).isin(test_set_json['conjuntos'])]


# Divide the test and train data (Teste com LIGHT)
# X = model_df.drop(columns=label)
# Y = model_df[label]
# X_train = X[~X.index.get_level_values(1).isin(test_set_json['conjuntos'])]
# X_test = X[X.index.get_level_values(1).isin(test_set_json['conjuntos'])]
# y_train = Y[~Y.index.get_level_values(1).isin(test_set_json['conjuntos'])]
# y_test = Y[Y.index.get_level_values(1).isin(test_set_json['conjuntos'])]

model = LinearRegression(fit_intercept=True)
model.fit(X_train, y_train)

X_train.shape, X_test.shape

y_test = pd.DataFrame(y_test)
y_test["y_p"] = model.predict(X_test)

y_test

fig, ax = plt.subplots()

ax.scatter(y_test[label], y_test["y_p"], color='gray')
ax.set_title("Quantidade de casos de interrupções real vs. inferência modelo - nov-dez/2022 - Regressao linear", loc='left',fontsize=10)

# r2 score
r2_score(y_test[label], y_test["y_p"])

MAPE(y_test[label], y_test.y_p)

hour = datetime.datetime.now().strftime("%Y_%m_%d_%H")

# Verifica a importancia de cada variavel na previsao, o modelo parece estar sobre ajustado pois os resultados foram acima do esperado para
# o modelo de regressao linear

feat_import_dic = {}
# Get feature_name and importance in a dict, and show
for feat, importance in zip(model.feature_names_in_, model.coef_):
  feat_import_dic[feat] =  importance

# Salva o dicionario de features importance em um excel
feat_import_df = pd.DataFrame(data=feat_import_dic, index=[0]).T.reset_index()
feat_import_df.columns = ["Feat", "Coef"]
feat_import_df.to_excel(base_path + "/Ordens_servico/feat_imp_linear_model_test_train_state_SE_{}.xlsx".format(hour))

dict(sorted(feat_import_dic.items(), key=lambda item: item[1]))
# feat_import_dic

"""## 3.4- Aplicacao de Modelo de Regressão nao linear

Apos o teste com a aplicacao modelo linear, aplica um modelo nao linear para verificar a qualidade do ajuste. Para o primeiro teste sera utilizado um modelo de arvore de decisao. Para o caso de uma variavel enteira de resposta a arvore de decisao gera a media das n resultados opos ser realizada a subdiviao.
"""

from sklearn.tree import DecisionTreeRegressor

"""Pontos importantes na arvore de decisao:
- Verificar o funcao de erro.
- Verifica se tem alguma coluna com valores nulos.

"""

# Verfica se existe algum outlier no conjunto de dados. Se ha eh necessario realizar um tratamento para a retirada dos valores nulos para o modelo.
df.isnull().any().to_list()

# Apply the next model (non-linear) - The linear model had a good result of r2, but is not feeting the problem very well, the MAPE is low
# We applie the Friedman RSME. - https://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=8954429&fileOId=8954430
model = DecisionTreeRegressor(random_state=2, criterion="friedman_mse")

# Its a time series, so for prediction we have to order the data by date (we will include districty too)
modeling_df = modeling_df.sort_values(["year_month"])

X = modeling_df.drop(columns="FLAG_INTERRUPCOES")
y = df["FLAG_INTERRUPCOES"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=30, shuffle=False)

model.fit(X_train, y_train)

X_test

X_test["y_pred"] = model.predict(X_test)
X_test["y_true"] = y_test



X_test[["y_pred", "y_true"]]



"""# 4 - Simulacão

Realiza a simulação referente a previsao da quantidade de interrupções para casos futuros. Diferentemente da seção anterior em que é realizada a comparação com casos passados, observados.

Definições
"""

variaveis_meteor = [
    'MAX_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)', 'MAX_PRESSAO ATMOSFERICA MEDIA DIARIA (AUT)(mB)', 'MAX_TEMPERATURA DO PONTO DE ORVALHO MEDIA DIARIA (AUT)(Â°C)',
       'MAX_TEMPERATURA MAXIMA, DIARIA (AUT)(Â°C)', 'MAX_TEMPERATURA MEDIA, DIARIA (AUT)(Â°C)','MAX_TEMPERATURA MINIMA, DIARIA (AUT)(Â°C)',
      'MAX_UMIDADE RELATIVA DO AR, MEDIA DIARIA (AUT)(%)','MAX_UMIDADE RELATIVA DO AR, MINIMA DIARIA (AUT)(%)','MAX_VENTO, RAJADA MAXIMA DIARIA (AUT)(m/s)',
       'MAX_VENTO, VELOCIDADE MEDIA DIARIA (AUT)(m/s)','MED_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)','MED_PRESSAO ATMOSFERICA MEDIA DIARIA (AUT)(mB)',
       'MED_TEMPERATURA DO PONTO DE ORVALHO MEDIA DIARIA (AUT)(Â°C)','MED_TEMPERATURA MAXIMA, DIARIA (AUT)(Â°C)','MED_TEMPERATURA MEDIA, DIARIA (AUT)(Â°C)',
       'MED_TEMPERATURA MINIMA, DIARIA (AUT)(Â°C)', 'MED_UMIDADE RELATIVA DO AR, MEDIA DIARIA (AUT)(%)','MED_UMIDADE RELATIVA DO AR, MINIMA DIARIA (AUT)(%)',
       'MED_VENTO, RAJADA MAXIMA DIARIA (AUT)(m/s)','MED_VENTO, VELOCIDADE MEDIA DIARIA (AUT)(m/s)','TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)'
]
variaveis_not_meteor = ['GEOCODIGO_MUNICIPIO',
       'NOME_MUNICIPIO', 'LONGITUDE', 'LATITUDE', 'CodMun', 'Mun', 'UF',
       'Pop estimada 2021', 'Faixa_pop', 'Região Metropolitana', 'Regiao',
       'Nome da Grande Região', 'Nome da Mesorregião', 'Nome da Microrregião',
       'Nome da Região Geográfica Imediata',
       'Município da Região Geográfica Imediata',
       'Nome da Região Geográfica Intermediária',
       'Município da Região Geográfica Intermediária',
       'Nome Concentração Urbana', 'Tipo Concentração Urbana',
       'Nome Arranjo Populacional', 'Hierarquia Urbana',
       'Hierarquia Urbana (principais categorias)', 'Nome da Região Rural',
       'Região rural (segundo classificação do núcleo)', 'Amazônia Legal',
       'Semiárido', 'Cidade-Região de São Paulo',
       'Valor adicionado bruto total, \na preços correntes\n(R$ 1.000)',
       'Impostos, líquidos de subsídios, sobre produtos, \na preços correntes\n(R$ 1.000)',
       'Produto Interno Bruto, \na preços correntes\n(R$ 1.000)',
       'Produto Interno Bruto per capita, \na preços correntes\n(R$ 1,00)',
       'FLAG_LIT', 'IdeConjUndConsumidoras', 'ds_munic', 'ds_uf',
       'MUN_MINIMUM_BOUND_RADIO', 'AREA', 'MUN_NUMBER_OF_BOUNDARYS',
       'veg_class', 'DscConjuntoUnidadeConsumidora',
       'NUMERO_CONSUMIDORES_DO_CONJUNTO', 'NUMERO_UNIDADES_CONSUMIDORAS',
       'FLAG_INTERRUPCOES', 'percent_tree_cover']

variaveis_meteor = dict.fromkeys(variaveis_meteor, 'mean')
variaveis_not_meteor = dict.fromkeys(variaveis_not_meteor, 'last')
d = {**variaveis_meteor, **variaveis_not_meteor}
d

"""## 4.1 - Anos Anteriores"""

# Leitura do conjunto de dados para o modelo
data_paths = list(dir_path.glob("Ordens_servico/base_2*"))

df = pd.concat([pd.read_excel(path, index_col=None) for path in data_paths])

df

"""Criando o conjunto de dados de 2023"""

# Esta sendo agrupado 3 meses de dados do conjunto. Porem caso nao haja dado para o ano anterior para contruir o lag, o conjunto sera retirado na inferencia.
df_fut = df[df.year.isin([2020, 2021, 2022])]
df_fut = df_fut.sort_values(by='year').groupby(['DscConjUndConsumidoras', 'mes']).agg(d).reset_index()
df_fut['year'] = 2023
# Input null for interruptions
df_fut["FLAG_INTERRUPCOES"] = np.nan
df_fut

df_fut = df_fut[df.drop(columns=['Unnamed: 0']).columns]
df = df.append(df_fut)

"""Criando o conjunto de dados de 2024"""

df_fut = df[df.year.isin([2021, 2022, 2023])]
df_fut = df_fut.sort_values(by='year').groupby(['DscConjUndConsumidoras', 'mes']).agg(d).reset_index()
df_fut['year'] = 2024
df_fut

df_fut = df_fut[df.drop(columns=['Unnamed: 0']).columns]
df = df.append(df_fut)

df

# Preprocess
df = treat_outliers(df)
df = fill_null(df)
df = add_clusters_info(df)
df = select_features(df, index, cols)
df = one_hot_econding(df)
df = drop_duplic(df)
df = transform_month(df)
df = fill_null_mean_tratment(df)
df = time_shift(df)
df = year_seasons(df)
df = norm_data(df)

"""### 4.1.1 - Energisa Sergipe"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='energisa_se', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='energisa_se', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json['conjuntos'])]
acc_se_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_se_df.plot()
acc_se_df

"""### 4.1.2 - Cemig"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='cemig', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='cemig', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])]
acc_cemig_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_cemig_df.plot()
acc_cemig_df

"""### 4.1.3 - Copel"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='copel', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='copel', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])]
acc_copel_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_copel_df.plot()
acc_copel_df

"""### 4.1.4 - CPFL"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='cpfl', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='cpfl', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])]
acc_cpfl_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_cpfl_df.plot()
acc_cpfl_df

"""### 4.1.5 - Persistencia do Resultado

Une todos os resultados das simulacoes com as distribuidoras e persiste
"""

utl_sim = pd.DataFrame({'energisa_sergipe': acc_se_df, 'cemig': acc_cemig_df, 'copel': acc_copel_df, 'cpfl': acc_cpfl_df})
utl_sim.to_excel((base_path + "Ordens_servico/sim_result.xlsx"))

"""Apos a persistencia realiza o plot do resultado final (O plot realiza a leitura novamente dos dados caso a seção seja fechada e reaberta)"""

utl_sim = pd.read_excel(base_path + "Ordens_servico/sim_result.xlsx")
utl_sim.year_month = utl_sim.year_month.astype('str')
utl_sim = utl_sim.set_index("year_month")
# transform index to datetime
utl_sim.index = pd.to_datetime(utl_sim.index, format='%Y%m')

from cycler import cycler
custom_marker_cycler = (cycler(marker=['o', 'x', 's', 'P']))

for idx, column_name in enumerate(utl_sim.columns):

  fig, ax = plt.subplots()

  ax.set_prop_cycle(custom_marker_cycler) # setting the cycler for the current figure

  # Plot the historic
  ax.plot(utl_sim[utl_sim[column_name].index <= pd.to_datetime('202212', format='%Y%m')][column_name], label=column_name)

  # create a new table equal to the utl_sim, chaging the name of the columns
  proj_df = utl_sim.rename(columns={column_name: column_name + " projection"})
  proj_column_name = column_name + " projection"

  # Plot the projection
  ax.plot(proj_df[proj_df[proj_column_name].index >= pd.to_datetime('202212', format='%Y%m')][proj_column_name], label=proj_column_name, linestyle='dashed')

  # ax.tick_params(axis='x', labelrotation = 90)
  # ax.set_xlabel('Date', fontsize = 12)
  # ax.set_ylabel('Number of outages', fontsize = 12)
  # Comment this part to set the figures title on inkspace
  ax.set_xlabel('', fontsize = 12)
  ax.set_ylabel('', fontsize = 12)

  for label in (ax.get_xticklabels() + ax.get_yticklabels()):
    label.set_fontsize(14)

  ax.legend(ncol=1, fontsize=11)

  # Save
  plt.savefig(base_path + "Ordens_servico/output/simulation_{}.svg".format(idx), format='svg')

  plt.show()

"""## 4.2 - El Nino

No ano de 1891, o senhor Dr. Luiz Carranza, presidente da sociedade geográfica de Lima, no Peru, contribuiu com um artigo para um boletim daquela Sociedade, chamando a atenção para o fato de que uma contra-corrente fluindo do norte para o sul foi observada entre Paita e Pacasmoyo.

Os marinheiros de Paita, que frequentemente navegavam pela costa em pequenas embarcações, tanto para o norte, quanto para o sul, apelidaram esta contra-corrente de El-nino (o Menino Jesus), porque ela era observada logo após as comemorações de Natal.

Dessa, forma, visando simular o el nino no Brasil, são projetos os meses de novembro, dezembro, janeiro e fevereiro. Levando em consideração, para os estados do norte e nordeste uma diminuição no regime de chuvas de 50 mm. E para os estados do Sul e Sudeste um regime de chuvas na faixa de 450 mm.
"""

# Leitura do conjunto de dados para o modelo
data_paths = list(dir_path.glob("Ordens_servico/base_2*"))

df = pd.concat([pd.read_excel(path, index_col=None) for path in data_paths])
# drop additional column
df = df.drop(columns='Unnamed: 0')

df

"""Modifica o conjunto de dados de 2023 e 2024.

Em 2023 e 2024  pega o valor médio dos útlimos 3 anos de precipitação e, para a previsão,acrescenta 300 mm nas distribuidoras:

- CPFL;
- Copel.

E retira 50 mm de precipitação das distribuidoras:
- Energisa Sergipe;
- Cemig.

Atributos Referentes a precipitação:

- MAX_PRECIPITACAO TOTAL, DIARIO (AUT)(mm);
- TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm);
- TOTAL, DIARIO (AUT)(mm)	MED_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)
"""

df["TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)"].hist()

# Estao sendo agrupados 3 meses de dados do conjunto. Porem caso nao haja dado para o ano anterior para contruir o lag, o conjunto sera retirado na inferencia.
# 2023 data
df_fut = df[df.year.isin([2020, 2021, 2022])]
df_fut = df_fut.sort_values(by='year').groupby(['DscConjUndConsumidoras', 'mes']).agg(d).reset_index()
df_fut['year'] = 2023
# Input null for interruptions
df_fut["FLAG_INTERRUPCOES"] = np.nan
df_fut
df = df.append(df_fut)

# 2024 data
df_fut = df[df.year.isin([2021, 2022, 2023])]
df_fut = df_fut.sort_values(by='year').groupby(['DscConjUndConsumidoras', 'mes']).agg(d).reset_index()
df_fut['year'] = 2024
df_fut
df = df.append(df_fut)

# Forca a precipitacao dos meses de nov, dez, jan e fev de 2023 e 2024, para 450 mm, para o sul e São paulo. (Copel - CPFL)
df["TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)"] = np.where(
    (df.year.isin([2023, 2024])) & (df.mes.isin([11, 12, 1, 2])) & (df.DscConjUndConsumidoras.isin( test_set_json_copel["conjuntos"] + test_set_json_cpfl["conjuntos"] )) ,
    450,
    df["TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)"]
)
# Forca a precipitacao dos meses de nov, dez, jan e fev de 2023 e 2024, para -50 mm, para o sudeste e nordeste (cemig - energisa se)
df["TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)"] = np.where(
    (df.year.isin([2023, 2024])) & (df.mes.isin([11, 12, 1, 2])) & (df.DscConjUndConsumidoras.isin( test_set_json["conjuntos"] + test_set_json_cemig["conjuntos"] )) ,
    df["TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)"] - 50,
    df["TOTAL_PRECIPITACAO TOTAL, DIARIO (AUT)(mm)"]
)

"""Preprocessamento"""

# Preprocess
df = treat_outliers(df)
df = fill_null(df)
df = add_clusters_info(df)
df = select_features(df, index, cols)
df = one_hot_econding(df)
df = drop_duplic(df)
df = transform_month(df)
df = fill_null_mean_tratment(df)
df = time_shift(df)
df = year_seasons(df)
df = norm_data(df)

"""### 4.2.1 Energisa Sergise"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='energisa_se', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='energisa_se', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json['conjuntos'])]
acc_nino_se_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_nino_se_df.plot()
acc_nino_se_df

# Plot simulacao El nino vs. periodo normal
# period_el_nino = [202211, 202212, 202301, 202302, 202311, 202312, 202401, 202402, 202411, 202412]
period_el_nino = [202211, 202212, 202301, 202302]

acc_se_df[acc_se_df.index.astype('int').isin(period_el_nino)].plot()
acc_nino_se_df[acc_nino_se_df.index.astype('int').isin(period_el_nino)].plot()

"""### 4.2.2 - Cemig"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='cemig', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='cemig', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json_cemig['conjuntos'])]
acc_nino_cemig_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_nino_cemig_df.plot()
acc_nino_cemig_df

# Plot simulacao El nino vs. periodo normal
# period_el_nino = [202211, 202212, 202301, 202302, 202311, 202312, 202401, 202402, 202411, 202412]
period_el_nino = [202211, 202212, 202301, 202302]

acc_cemig_df[acc_cemig_df.index.astype('int').isin(period_el_nino)].plot()
acc_nino_cemig_df[acc_nino_cemig_df.index.astype('int').isin(period_el_nino)].plot()

"""### 4.2.3 - Copel"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='copel', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='copel', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json_copel['conjuntos'])]
acc_nino_copel_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_nino_copel_df.plot()
acc_nino_copel_df

# Plot simulacao El nino vs. periodo normal
# period_el_nino = [202211, 202212, 202301, 202302, 202311, 202312, 202401, 202402, 202411, 202412]
period_el_nino = [202211, 202212, 202301, 202302]

acc_copel_df[acc_copel_df.index.astype('int').isin(period_el_nino)].plot()
acc_nino_copel_df[acc_nino_copel_df.index.astype('int').isin(period_el_nino)].plot()

"""### 4.3.4 - CPFL"""

# Previsao para 2023 - Retira o ano de 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df[~df.index.get_level_values(0).astype('str').str.contains("2024")], separe_type='cpfl', testyear='2023')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original, e faz o lag de 2024.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

df = time_shift(df)

# Previsao para 2024
model, X_train, X_test, y_train, y_test = sep_train_test(df, separe_type='cpfl', testyear='2024')
# X_test = X_test.dropna()
y_test = inference(X_test, y_test, simulation=True)
y_test

# Coloca as projecoes previstas como reais no conjunto original.
df = df.join(y_test["y_p"])
df["FLAG_INTERRUPCOES"] = np.where(
    df["y_p"].isnull(),
    df["FLAG_INTERRUPCOES"],
    df["y_p"]
)
df = df.drop(columns="y_p")

# Plot da serie temporal do acumulado da quantidade de interrupcoes
plt.figure(figsize=(12, 4))

utl_df = df[df.index.get_level_values(1).isin(test_set_json_cpfl['conjuntos'])]
acc_nino_cpfl_df = utl_df.reset_index().groupby('year_month')["FLAG_INTERRUPCOES"].sum()
acc_nino_cpfl_df.plot()
acc_nino_cpfl_df

acc_cpfl_df.index.astype('int').isin([202211, 202212, 202301, 202302, 202311, 202312, 202401, 202402, 202411, 202412])

# Plot simulacao El nino vs. periodo normal
acc_cpfl_df.plot()
acc_nino_cpfl_df.plot()

# Plot simulacao El nino vs. periodo normal
# period_el_nino = [202211, 202212, 202301, 202302, 202311, 202312, 202401, 202402, 202411, 202412]
period_el_nino = [202211, 202212, 202301, 202302]

acc_cpfl_df[acc_cpfl_df.index.astype('int').isin(period_el_nino)].plot()
acc_nino_cpfl_df[acc_nino_cpfl_df.index.astype('int').isin(period_el_nino)].plot()

"""### 4.2.5 - Persistencia do Resultado"""

utl_sim_el_nino = pd.DataFrame({'energisa_sergipe': acc_nino_se_df, 'cemig': acc_nino_cemig_df, 'copel': acc_nino_copel_df, 'cpfl': acc_nino_cpfl_df})
utl_sim_el_nino.to_excel((base_path + "Ordens_servico/sim_el_nino_result.xlsx"))

"""Plota o resultado final da simulacão do periodo normal e com o evento de el nino"""

utl_sim = pd.read_excel(base_path + "Ordens_servico/sim_result.xlsx")
utl_sim.year_month = utl_sim.year_month.astype('str')
utl_sim = utl_sim.set_index("year_month")
# transform index to datetime
utl_sim.index = pd.to_datetime(utl_sim.index, format='%Y%m')

utl_sim_el_nino = pd.read_excel((base_path + "Ordens_servico/sim_el_nino_result.xlsx"))
utl_sim_el_nino.year_month = utl_sim_el_nino.year_month.astype('str')
utl_sim_el_nino = utl_sim_el_nino.set_index("year_month")
# transform index to datetime
utl_sim_el_nino.index = pd.to_datetime(utl_sim_el_nino.index, format='%Y%m')

"""Filtra somente os meses em que o el nino tem influencia"""

from cycler import cycler
custom_marker_cycler = (cycler(marker=['o', 'x', 's', 'P']))

for idx, column_name in enumerate(utl_sim.columns):

  fig, ax = plt.subplots(figsize=(8,8))

  ax.set_prop_cycle(custom_marker_cycler) # setting the cycler for the current figure

  # Plot the historic
  # ax.plot(utl_sim[utl_sim[column_name].index <= pd.to_datetime('202212', format='%Y%m')][column_name], label=column_name)
  # ax.plot(utl_sim_el_nino[utl_sim_el_nino[column_name].index <= pd.to_datetime('202212', format='%Y%m')][column_name], label=column_name)

  # create a new table equal to the utl_sim, chaging the name of the columns
  # proj_df = utl_sim.rename(columns={column_name: column_name + " projection"})
  # proj_el_nino_df = utl_sim.rename(columns={column_name: column_name + " projection"})
  # proj_column_name = column_name + " projection"

  # Plot the projection
  ax.plot(utl_sim[utl_sim[column_name].index >= pd.to_datetime('202212', format='%Y%m')][column_name], label=column_name, linestyle='solid')
  ax.plot(utl_sim_el_nino[utl_sim_el_nino[column_name].index >= pd.to_datetime('202212', format='%Y%m')][column_name], label=column_name, linestyle='dotted', color='gray')

  # ax.tick_params(axis='x', labelrotation = 90)
  # ax.set_xlabel('Date', fontsize = 12)
  # ax.set_ylabel('Number of outages', fontsize = 12)
  # Comment this part to set the figures title on inkspace
  ax.set_xlabel('', fontsize = 12)
  ax.set_ylabel('', fontsize = 12)

  for label in (ax.get_xticklabels() + ax.get_yticklabels()):
    label.set_fontsize(14)

  ax.legend([column_name, column_name + '_el_nino'],ncol=1, fontsize=11)
  ax.tick_params(axis='x', labelrotation=90)

  # Save
  plt.savefig(base_path + "Ordens_servico/output/simulation_{}.svg".format(idx), format='svg')

  plt.show()

